{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://towardsdatascience.com/building-a-python-code-generator-4b476eec5804\n",
        "https://github.com/divyam96/English-to-Python-Converter/blob/main/English_to_Python.ipynb\n",
        "https://towardsdatascience.com/custom-datasets-in-pytorch-part-2-text-machine-translation-71c41a3e994e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import os\n",
        "from tempfile import TemporaryDirectory\n",
        "from typing import Tuple\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch import Tensor\n",
        "from torch.utils.data import dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = \"cpu\"\n",
        "print(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[' write a python program to add two numbers \\n', \"num1 = 1.5\\nnum2 = 6.3\\nsum = num1 + num2\\nprint(f'Sum: {sum}')\\n\\n\\n\"]\n"
          ]
        }
      ],
      "source": [
        "f = open(\"english_python_data.txt\", \"r\")\n",
        "file_lines = f.readlines()\n",
        "#Build Dataset\n",
        "dps = []\n",
        "dp = None\n",
        "questions = []\n",
        "solutions = []\n",
        "for line in file_lines:\n",
        "  if line[0] == \"#\":\n",
        "    if dp:\n",
        "      dp['solution'] = ''.join(dp['solution'])\n",
        "      dps.append(dp)\n",
        "      solutions.append(dp['solution'])\n",
        "    dp = {\"question\": None, \"solution\": []}\n",
        "    dp['question'] = line[1:]\n",
        "    questions.append(line[1:])\n",
        "  else:\n",
        "    dp[\"solution\"].append(line)\n",
        "  \n",
        "print([[x,y] for x,y in zip(questions,solutions)][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "from tokenize import tokenize, untokenize\n",
        "import io\n",
        "def tokenize_python_code(python_code_str):\n",
        "    python_tokens = list(tokenize(io.BytesIO(python_code_str.encode('utf-8')).readline))\n",
        "    tokenized_output = []\n",
        "    for i in range(0, len(python_tokens)):\n",
        "        tokenized_output.append((python_tokens[i].type, python_tokens[i].string))\n",
        "    return tokenized_output\n",
        "\n",
        "#print(dps[0])\n",
        "error_count = 0\n",
        "errors = []\n",
        "for i in solutions:\n",
        "    try:\n",
        "        tokenize_python_code(i)\n",
        "    except Exception as e:\n",
        "        errors.append((e,i))\n",
        "        error_count += 1\n",
        "\n",
        "print(error_count)\n",
        "for e,i in errors:\n",
        "    print(e)\n",
        "    print(i)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>solution</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>write a python program to add two numbers \\n</td>\n",
              "      <td>num1 = 1.5\\nnum2 = 6.3\\nsum = num1 + num2\\npri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>write a python function to add two user provi...</td>\n",
              "      <td>def add_two_numbers(num1, num2):\\n    sum = nu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>write a program to find and print the largest...</td>\n",
              "      <td>\\nnum1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 &gt;=...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>write a program to find and print the smalles...</td>\n",
              "      <td>num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 &lt;= n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Write a python function to merge two given li...</td>\n",
              "      <td>def merge_lists(l1, l2):\\n    return l1 + l2\\n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question   \n",
              "0       write a python program to add two numbers \\n  \\\n",
              "1   write a python function to add two user provi...   \n",
              "2   write a program to find and print the largest...   \n",
              "3   write a program to find and print the smalles...   \n",
              "4   Write a python function to merge two given li...   \n",
              "\n",
              "                                            solution  \n",
              "0  num1 = 1.5\\nnum2 = 6.3\\nsum = num1 + num2\\npri...  \n",
              "1  def add_two_numbers(num1, num2):\\n    sum = nu...  \n",
              "2  \\nnum1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 >=...  \n",
              "3  num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 <= n...  \n",
              "4  def merge_lists(l1, l2):\\n    return l1 + l2\\n...  "
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "python_problems_df = pd.DataFrame(dps)\n",
        "python_problems_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "question     Write a program to print the multiplication t...\n",
              "solution    \\nnum = 9\\nfor i in range(1, 11):\\n   print(f\"...\n",
              "Name: 9, dtype: object"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "python_problems_df.loc[9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = []\n",
        "val_df = []\n",
        "np.random.seed(0)\n",
        "msk = np.random.rand(len(dps)) < .85 # Splitting data into 85% train and 15% validation\n",
        "for truth,val in zip(msk,dps):\n",
        "    if truth:\n",
        "        train_df.append(val)\n",
        "    else:\n",
        "        val_df.append(val)\n",
        "\n",
        "train_df = pd.DataFrame(train_df)\n",
        "val_df = pd.DataFrame(val_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>solution</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Write a program to check whether a number is ...</td>\n",
              "      <td>num = 337\\n\\nif num &gt; 1:\\n   for i in range(2,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Write a python function that prints the facto...</td>\n",
              "      <td>def print_factors(x):\\n   print(f\"The factors ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Write a program to print the multiplication t...</td>\n",
              "      <td>\\nnum = 9\\nfor i in range(1, 11):\\n   print(f\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Write a python function to print powers of 2,...</td>\n",
              "      <td>def two_power(terms):\\n    result = list(map(l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Write a program to filter the numbers in a li...</td>\n",
              "      <td>my_list = [11, 45, 74, 89, 132, 239, 721, 21]\\...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question   \n",
              "5   Write a program to check whether a number is ...  \\\n",
              "6   Write a python function that prints the facto...   \n",
              "7   Write a program to print the multiplication t...   \n",
              "8   Write a python function to print powers of 2,...   \n",
              "9   Write a program to filter the numbers in a li...   \n",
              "\n",
              "                                            solution  \n",
              "5  num = 337\\n\\nif num > 1:\\n   for i in range(2,...  \n",
              "6  def print_factors(x):\\n   print(f\"The factors ...  \n",
              "7  \\nnum = 9\\nfor i in range(1, 11):\\n   print(f\"...  \n",
              "8  def two_power(terms):\\n    result = list(map(l...  \n",
              "9  my_list = [11, 45, 74, 89, 132, 239, 721, 21]\\...  "
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df[5:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Vocabulary:\n",
        "  \n",
        "    def __init__(self, freq_threshold, max_size):\n",
        "        '''\n",
        "        freq_threshold : the minimum times a word must occur in corpus to be treated in vocab\n",
        "        max_size : max source vocab size. Eg. if set to 10,000, we pick the top 10,000 most frequent words and discard others\n",
        "        '''\n",
        "        #initiate the index to token dict\n",
        "        ## <PAD> -> padding, used for padding the shorter sentences in a batch to match the length of longest sentence in the batch\n",
        "        ## <SOS> -> start token, added in front of each sentence to signify the start of sentence\n",
        "        ## <EOS> -> End of sentence token, added to the end of each sentence to signify the end of sentence\n",
        "        ## <UNK> -> words which are not found in the vocab are replace by this token\n",
        "        self.itos = {0: '<PAD>', 1:'<SOS>', 2:'<EOS>', 3: '<UNK>'}\n",
        "        #initiate the token to index dict\n",
        "        self.stoi = {k:j for j,k in self.itos.items()} \n",
        "        \n",
        "        self.freq_threshold = freq_threshold\n",
        "        self.max_size = max_size\n",
        "    \n",
        "    '''\n",
        "    __len__ is used by dataloader later to create batches\n",
        "    '''\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "    \n",
        "    '''\n",
        "    a simple tokenizer to split on space and converts the sentence to list of words\n",
        "    '''\n",
        "    @staticmethod\n",
        "    def tokenizer(text):\n",
        "        return [tok.lower().strip() for tok in text.split(' ')]\n",
        "    \n",
        "    '''\n",
        "    build the vocab: create a dictionary mapping of index to string (itos) and string to index (stoi)\n",
        "    output ex. for stoi -> {'the':5, 'a':6, 'an':7}\n",
        "    '''\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        pass\n",
        "            \n",
        "    '''\n",
        "    convert the list of words to a list of corresponding indexes\n",
        "    '''    \n",
        "    def numericalize_base(self, tokenized_text):\n",
        "        numericalized_text = []\n",
        "        for token in tokenized_text:\n",
        "            if token in self.stoi.keys():\n",
        "                numericalized_text.append(self.stoi[token])\n",
        "            else: #out-of-vocab words are represented by UNK token index\n",
        "                numericalized_text.append(self.stoi['<UNK>'])\n",
        "                \n",
        "        return numericalized_text\n",
        "\n",
        "class Vocabulary_Output(Vocabulary):\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        \n",
        "\n",
        "        frequencies = {}  #init the freq dict\n",
        "        idx = 4 #index from which we want our dict to start. We already used 4 indexes for pad, start, end, unk\n",
        "        \n",
        "        for sentence in sentence_list:\n",
        "            python_tokens = list(tokenize(io.BytesIO(sentence.encode('utf-8')).readline))\n",
        "            tokenized_output = []\n",
        "            #print([i.string for i in list(tokenize(io.BytesIO(sentence.encode('utf-8')).readline))])\n",
        "            for i in range(0, len(python_tokens)):\n",
        "                word = python_tokens[i].string\n",
        "                # print(word)\n",
        "                if word not in frequencies.keys():\n",
        "                    frequencies[word]=1\n",
        "                else:\n",
        "                    frequencies[word]+=1\n",
        "\n",
        "        #limit vocab by removing low freq words\n",
        "        frequencies = {k:v for k,v in frequencies.items() if v>self.freq_threshold} \n",
        "        \n",
        "        print(self.freq_threshold)\n",
        "\n",
        "        #limit vocab to the max_size specified\n",
        "        frequencies = dict(sorted(frequencies.items(), key = lambda x: -x[1])[:self.max_size-idx]) # idx =4 for pad, start, end , unk\n",
        "        \n",
        "        #create vocab\n",
        "        for word in frequencies.keys():\n",
        "            self.stoi[word] = idx\n",
        "            self.itos[idx] = word\n",
        "            idx+=1\n",
        "    \n",
        "    def numericalize(self, text):\n",
        "        tokenized_text = [i.string for i in list(tokenize(io.BytesIO(text.encode('utf-8')).readline))]\n",
        "        return self.numericalize_base(tokenized_text)\n",
        "                \n",
        "class Vocabulary_Input(Vocabulary):\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        #calculate the frequencies of each word first to remove the words with freq < freq_threshold\n",
        "        frequencies = {}  #init the freq dict\n",
        "        idx = 4 #index from which we want our dict to start. We already used 4 indexes for pad, start, end, unk\n",
        "        \n",
        "        #calculate freq of words\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer(sentence):\n",
        "                if word not in frequencies.keys():\n",
        "                    frequencies[word]=1\n",
        "                else:\n",
        "                    frequencies[word]+=1\n",
        "                    \n",
        "                    \n",
        "        #limit vocab by removing low freq words\n",
        "        frequencies = {k:v for k,v in frequencies.items() if v>self.freq_threshold} \n",
        "        \n",
        "        #limit vocab to the max_size specified\n",
        "        frequencies = dict(sorted(frequencies.items(), key = lambda x: -x[1])[:self.max_size-idx]) # idx =4 for pad, start, end , unk\n",
        "        \n",
        "        \n",
        "\n",
        "        #create vocab\n",
        "        for word in frequencies.keys():\n",
        "            self.stoi[word] = idx\n",
        "            self.itos[idx] = word\n",
        "            idx+=1\n",
        "    def numericalize(self, text):\n",
        "        tokenized_text = self.tokenizer(text)\n",
        "        return self.numericalize_base(tokenized_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "3381\n",
            " write a python function to add two user provided numbers and return the sum\n",
            "\n",
            "def add_two_numbers(num1, num2):\n",
            "    sum = num1 + num2\n",
            "    return sum\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Test\n",
        "voc = Vocabulary_Input(0,100000)\n",
        "voc.build_vocabulary(questions)\n",
        "#print(voc.stoi)\n",
        "#print(voc.numericalize(\"write a python program to add two numbers\"))\n",
        "\n",
        "voc_out = Vocabulary_Output(0,10000)\n",
        "voc_out.build_vocabulary(solutions)\n",
        "#print(voc_out.itos)\n",
        "\n",
        "print(len(voc_out.itos))\n",
        "\n",
        "\n",
        "\n",
        "print(questions[1])\n",
        "print(solutions[1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n"
          ]
        }
      ],
      "source": [
        "print(voc_out.stoi['('])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "class Train_Dataset(Dataset):\n",
        "    '''\n",
        "    Initiating Variables\n",
        "    df: the training dataframe\n",
        "    source_column : the name of source text column in the dataframe\n",
        "    target_columns : the name of target text column in the dataframe\n",
        "    transform : If we want to add any augmentation\n",
        "    freq_threshold : the minimum times a word must occur in corpus to be treated in vocab\n",
        "    source_vocab_max_size : max source vocab size\n",
        "    target_vocab_max_size : max target vocab size\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, df, source_column, target_column, transform=None, freq_threshold = 0,\n",
        "                source_vocab_max_size = 10000, target_vocab_max_size = 10000):\n",
        "    \n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        \n",
        "        #get source and target texts\n",
        "        self.source_texts = self.df[source_column]\n",
        "        self.target_texts = self.df[target_column]\n",
        "        \n",
        "        \n",
        "        ##VOCAB class has been created above\n",
        "        #Initialize source vocab object and build vocabulary\n",
        "        self.source_vocab = Vocabulary_Input(freq_threshold, source_vocab_max_size)\n",
        "        self.source_vocab.build_vocabulary(self.source_texts.tolist())\n",
        "        #Initialize target vocab object and build vocabulary\n",
        "        self.target_vocab = Vocabulary_Output(freq_threshold, target_vocab_max_size)\n",
        "        self.target_vocab.build_vocabulary(self.target_texts.tolist())\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    '''\n",
        "    __getitem__ runs on 1 example at a time. Here, we get an example at index and return its numericalize source and\n",
        "    target values using the vocabulary objects we created in __init__\n",
        "    '''\n",
        "    def __getitem__(self, index):\n",
        "        source_text = self.source_texts[index]\n",
        "        target_text = self.target_texts[index]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            source_text = self.transform(source_text)\n",
        "            \n",
        "        #numericalize texts ['<SOS>','cat', 'in', 'a', 'bag','<EOS>'] -> [1,12,2,9,24,2]\n",
        "        numerialized_source = [self.source_vocab.stoi[\"<SOS>\"]]\n",
        "        numerialized_source += self.source_vocab.numericalize(source_text)\n",
        "        numerialized_source.append(self.source_vocab.stoi[\"<EOS>\"])\n",
        "    \n",
        "        numerialized_target = [self.target_vocab.stoi[\"<SOS>\"]]\n",
        "        numerialized_target += self.target_vocab.numericalize(target_text)\n",
        "        numerialized_target.append(self.target_vocab.stoi[\"<EOS>\"])\n",
        "        \n",
        "        #convert the list to tensor and return\n",
        "        return torch.tensor(numerialized_source), torch.tensor(numerialized_target) \n",
        "\n",
        "class Validation_Dataset:\n",
        "    def __init__(self, train_dataset, df, source_column, target_column, transform = None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        \n",
        "        #train dataset will be used as lookup for vocab\n",
        "        self.train_dataset = train_dataset\n",
        "        \n",
        "        #get source and target texts\n",
        "        self.source_texts = self.df[source_column]\n",
        "        self.target_texts = self.df[target_column]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "        source_text = self.source_texts[index]\n",
        "        #print(source_text)\n",
        "        target_text = self.target_texts[index]\n",
        "        #print(target_text)\n",
        "        if self.transform is not None:\n",
        "            source_text = self.transform(source_text)\n",
        "            \n",
        "        #numericalize texts ['<SOS>','cat', 'in', 'a', 'bag','<EOS>'] -> [1,12,2,9,24,2]\n",
        "        numerialized_source = [self.train_dataset.source_vocab.stoi[\"<SOS>\"]]\n",
        "        numerialized_source += self.train_dataset.source_vocab.numericalize(source_text)\n",
        "        numerialized_source.append(self.train_dataset.source_vocab.stoi[\"<EOS>\"])\n",
        "     \n",
        "        numerialized_target = [self.train_dataset.target_vocab.stoi[\"<SOS>\"]]\n",
        "        numerialized_target += self.train_dataset.target_vocab.numericalize(target_text)\n",
        "        numerialized_target.append(self.train_dataset.target_vocab.stoi[\"<EOS>\"])\n",
        "        #print(numerialized_source)\n",
        "        return torch.tensor(numerialized_source), torch.tensor(numerialized_target) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "question     Write a python function that returns the sum ...\n",
            "solution    def sum_natural(num):\\n    if num < 0:\\n      ...\n",
            "Name: 10, dtype: object\n",
            "(tensor([  1,   5,   7,   4,   8,  11,  19,  37,   9,  51,  10,  41, 114,  24,\n",
            "          2]), tensor([   1,   11,   19, 1400,    5,   37,    6,    9,    4,   17,   23,   37,\n",
            "          64,   22,    9,    4,   94,   14,    5, 1401,    6,    4,    7,   35,\n",
            "           9,    4,   94,   71,   10,   22,    4,   68,    5,   37,   58,   22,\n",
            "           6,    9,    4,  182,   71,   59,   37,    4,   37,  224,   15,    4,\n",
            "           7,   20,   37,    4,    4,    7,    7,    7,    2]))\n",
            "15 57\n",
            "['<SOS>', '', 'write', 'a', 'python', 'function', 'that', 'returns', 'the', 'sum', 'of', 'n', 'natural', 'numbers', '<EOS>']\n",
            "['<SOS>', 'utf-8', 'def', 'sum_natural', '(', 'num', ')', ':', '\\n', '    ', 'if', 'num', '<', '0', ':', '\\n', '       ', 'print', '(', '\"Please enter a positive number!\"', ')', '\\n', '', 'else', ':', '\\n', '       ', 'sum', '=', '0', '\\n', 'while', '(', 'num', '>', '0', ')', ':', '\\n', '           ', 'sum', '+=', 'num', '\\n', 'num', '-=', '1', '\\n', '', 'return', 'num', '\\n', '\\n', '', '', '', '<EOS>']\n"
          ]
        }
      ],
      "source": [
        "train_dataset = Train_Dataset(train_df, 'question', 'solution')\n",
        "idx = 10\n",
        "print(train_df.loc[idx])\n",
        "print(train_dataset[idx])\n",
        "\n",
        "print(len(train_dataset[idx][0]), len(train_dataset[idx][1]))\n",
        "\n",
        "print([train_dataset.source_vocab.itos[i.item()] for i in train_dataset[idx][0]])\n",
        "print([train_dataset.target_vocab.itos[i.item()] for i in train_dataset[idx][1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([37])\n",
            "tensor([[0.0638, 0.2659, 0.1415,  ..., 0.7142, 0.5426, 0.2871],\n",
            "        [0.2370, 0.4653, 0.9684,  ..., 0.9255, 0.8317, 0.3114],\n",
            "        [0.7673, 0.6815, 0.8197,  ..., 0.1055, 0.5047, 0.3394],\n",
            "        ...,\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n",
            "torch.Size([50, 100])\n"
          ]
        }
      ],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Desired max length\n",
        "max_len = 50\n",
        "\n",
        "# 100 seqs of variable length (< max_len)\n",
        "seq_lens = torch.randint(low=10,high=44,size=(100,))\n",
        "seqs = [torch.rand(n) for n in seq_lens]\n",
        "\n",
        "print(seqs[0].shape)\n",
        "\n",
        "# pad first seq to desired length\n",
        "seqs[0] = nn.ConstantPad1d((0, max_len - seqs[0].shape[0]), 0)(seqs[0])\n",
        "\n",
        "# pad all seqs to desired length\n",
        "seqs = pad_sequence(seqs)\n",
        "print(seqs)\n",
        "print(seqs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyCollate:\n",
        "    def __init__(self, pad_idx):\n",
        "        self.pad_idx = pad_idx\n",
        "        \n",
        "    #__call__: a default method\n",
        "    ##   First the obj is created using MyCollate(pad_idx) in data loader\n",
        "    ##   Then if obj(batch) is called -> __call__ runs by default\n",
        "    def __call__(self, batch):\n",
        "        #get all source indexed sentences of the batch\n",
        "\n",
        "\n",
        "        source = [item[0] for item in batch] \n",
        "        #pad them using pad_sequence method from pytorch. \n",
        "        source = torch.nn.utils.rnn.pad_sequence(source, batch_first=False, padding_value = self.pad_idx) \n",
        "        \n",
        "        #get all target indexed sentences of the batch\n",
        "        target = [item[1] for item in batch] \n",
        "        #pad them using pad_sequence method from pytorch. \n",
        "        target = torch.nn.utils.rnn.pad_sequence(target, batch_first=False, padding_value = self.pad_idx)\n",
        "        return source, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If we run a next(iter(data_loader)) we get an output of batch_size * (num_workers+1)\n",
        "def get_train_loader(dataset, batch_size, num_workers=0, shuffle=True, pin_memory=True): #increase num_workers according to CPU\n",
        "    #get pad_idx for collate fn\n",
        "    pad_idx = dataset.source_vocab.stoi['<PAD>']\n",
        "    #define loader\n",
        "    loader = DataLoader(dataset, batch_size = batch_size, num_workers = num_workers,\n",
        "                        shuffle=shuffle,\n",
        "                       pin_memory=pin_memory, collate_fn = MyCollate(pad_idx=pad_idx)) #MyCollate class runs __call__ method by default\n",
        "    return loader\n",
        "\n",
        "def get_valid_loader(dataset, train_dataset, batch_size, num_workers=0, shuffle=True, pin_memory=True):\n",
        "    pad_idx = train_dataset.source_vocab.stoi['<PAD>']\n",
        "    loader = DataLoader(dataset, batch_size = batch_size, num_workers = num_workers,\n",
        "                        shuffle=shuffle,\n",
        "                       pin_memory=pin_memory, collate_fn = MyCollate(pad_idx=pad_idx))\n",
        "    return loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([  1,   5,   7,   4,  12,   6,  16,   9, 146, 272,  10,   4,  18,  17,\n",
              "           2]),\n",
              " tensor([   1,   11,    4,   37,   10,  110,    4,   27,   18,   24,   39,    5,\n",
              "           15,    8,  183,    6,    9,    4,   83,   14,    5, 1390,    6,    4,\n",
              "            4,    4,    7,    7,    2]))"
            ]
          },
          "execution_count": 208,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "source: \n",
            " tensor([[  1],\n",
            "        [  5],\n",
            "        [  7],\n",
            "        [  4],\n",
            "        [  8],\n",
            "        [ 12],\n",
            "        [  6],\n",
            "        [134],\n",
            "        [ 34],\n",
            "        [  9],\n",
            "        [107],\n",
            "        [ 15],\n",
            "        [  4],\n",
            "        [ 52],\n",
            "        [  2]])\n",
            "target: \n",
            " tensor([[   1],\n",
            "        [  11],\n",
            "        [   4],\n",
            "        [  19],\n",
            "        [1403],\n",
            "        [   5],\n",
            "        [ 129],\n",
            "        [   8],\n",
            "        [ 166],\n",
            "        [   6],\n",
            "        [   9],\n",
            "        [   4],\n",
            "        [  17],\n",
            "        [  20],\n",
            "        [  52],\n",
            "        [   5],\n",
            "        [ 141],\n",
            "        [   5],\n",
            "        [ 129],\n",
            "        [   8],\n",
            "        [ 166],\n",
            "        [   6],\n",
            "        [   6],\n",
            "        [   4],\n",
            "        [   4],\n",
            "        [   4],\n",
            "        [   7],\n",
            "        [   7],\n",
            "        [   2]])\n",
            "source shape:  torch.Size([15, 1])\n",
            "target shape:  torch.Size([29, 1])\n"
          ]
        }
      ],
      "source": [
        "train_loader = get_train_loader(train_dataset, 1)\n",
        "\n",
        "source = next(iter(train_loader))[0]\n",
        "target = next(iter(train_loader))[1]\n",
        "\n",
        "print('source: \\n', source)\n",
        "print('target: \\n', target)\n",
        "\n",
        "print('source shape: ',source.shape)\n",
        "print('target shape: ', target.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_size,\n",
        "        src_vocab_size,\n",
        "        trg_vocab_size,\n",
        "        src_pad_idx,\n",
        "        num_heads,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        max_len,\n",
        "        device,\n",
        "    ):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
        "        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\n",
        "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n",
        "        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\n",
        "\n",
        "        self.device = device\n",
        "        self.transformer = nn.Transformer(\n",
        "            embedding_size,\n",
        "            num_heads,\n",
        "            num_encoder_layers,\n",
        "            num_decoder_layers,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "            batch_first=False\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = src.transpose(0, 1) == self.src_pad_idx\n",
        "\n",
        "        # (N, src_len)\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_seq_length, N = src.shape\n",
        "        trg_seq_length, N = trg.shape\n",
        "\n",
        "        src_positions = (\n",
        "            torch.arange(0, src_seq_length)\n",
        "            .unsqueeze(1)\n",
        "            .expand(src_seq_length, N)\n",
        "            .to(self.device)\n",
        "        )\n",
        "\n",
        "        trg_positions = (\n",
        "            torch.arange(0, trg_seq_length)\n",
        "            .unsqueeze(1)\n",
        "            .expand(trg_seq_length, N)\n",
        "            .to(self.device)\n",
        "        )\n",
        "\n",
        "        embed_src = self.dropout(\n",
        "            (self.src_word_embedding(src) + self.src_position_embedding(src_positions))\n",
        "        )\n",
        "        embed_trg = self.dropout(\n",
        "            (self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions))\n",
        "        )\n",
        "\n",
        "        src_padding_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(\n",
        "            self.device\n",
        "        )\n",
        "        print(\"here\")\n",
        "        out = self.transformer(\n",
        "            embed_src,\n",
        "            embed_trg,\n",
        "            src_key_padding_mask=src_padding_mask,\n",
        "            tgt_mask=trg_mask,\n",
        "        )\n",
        "        out = self.fc_out(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We're ready to define everything we need for training our Seq2Seq model\n",
        "\n",
        "load_model = False\n",
        "save_model = False\n",
        "\n",
        "# Training hyperparameters\n",
        "num_epochs = 10000\n",
        "learning_rate = 3e-4\n",
        "batch_size = 1\n",
        "\n",
        "# Model hyperparameters\n",
        "src_vocab_size = len(train_dataset.source_vocab)\n",
        "trg_vocab_size = len(train_dataset.target_vocab)\n",
        "embedding_size = 512\n",
        "num_heads = 8\n",
        "num_encoder_layers = 3\n",
        "num_decoder_layers = 3\n",
        "dropout = 0.10\n",
        "max_len = 100\n",
        "forward_expansion = 4\n",
        "src_pad_idx = train_dataset.source_vocab.stoi[\"<PAD>\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Transformer(\n",
        "    embedding_size,\n",
        "    src_vocab_size,\n",
        "    trg_vocab_size,\n",
        "    src_pad_idx,\n",
        "    num_heads,\n",
        "    num_encoder_layers,\n",
        "    num_decoder_layers,\n",
        "    forward_expansion,\n",
        "    dropout,\n",
        "    max_len,\n",
        "    device,\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {},
      "outputs": [],
      "source": [
        "writer = SummaryWriter(\"runs/loss_plot\")\n",
        "step = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, factor=0.1, patience=10, verbose=True\n",
        ")\n",
        "pad_idx = train_dataset.source_vocab.stoi[\"<PAD>\"]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 0 / 10000]\n",
            "batch idx  0\n",
            "torch.Size([18, 1]) torch.Size([31, 1])\n",
            "18\n",
            "torch.Size([512, 1]) torch.Size([512, 1])\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "index out of range in self",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[269], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mprint\u001b[39m(inp_data\u001b[39m.\u001b[39mshape,target\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     37\u001b[0m target[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[0;32m---> 38\u001b[0m output \u001b[39m=\u001b[39m model(inp_data,target )\n\u001b[1;32m     39\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39moutput:\u001b[39m\u001b[39m\"\u001b[39m,output\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     41\u001b[0m \u001b[39m# Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m# doesn't take input in that form. For example if we have MNIST we want to have\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m# output to be: (N, 10) and targets just (N). Here we can view it in a similar\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m# way that we have output_words * batch_size that we want to send in into\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39m# our cost function, so we need to do some reshapin.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m# Let's also remove the start token while we're at it\u001b[39;00m\n",
            "File \u001b[0;32m~/Code_Workspace/Python/CodeSematicSearch/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn[264], line 61\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     46\u001b[0m src_positions \u001b[39m=\u001b[39m (\n\u001b[1;32m     47\u001b[0m     torch\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, src_seq_length)\n\u001b[1;32m     48\u001b[0m     \u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[39m.\u001b[39mexpand(src_seq_length, N)\n\u001b[1;32m     50\u001b[0m     \u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     53\u001b[0m trg_positions \u001b[39m=\u001b[39m (\n\u001b[1;32m     54\u001b[0m     torch\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, trg_seq_length)\n\u001b[1;32m     55\u001b[0m     \u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     56\u001b[0m     \u001b[39m.\u001b[39mexpand(trg_seq_length, N)\n\u001b[1;32m     57\u001b[0m     \u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     58\u001b[0m )\n\u001b[1;32m     60\u001b[0m embed_src \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\n\u001b[0;32m---> 61\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_word_embedding(src) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msrc_position_embedding(src_positions))\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     63\u001b[0m embed_trg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\n\u001b[1;32m     64\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrg_word_embedding(trg) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrg_position_embedding(trg_positions))\n\u001b[1;32m     65\u001b[0m )\n\u001b[1;32m     67\u001b[0m src_padding_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_src_mask(src)\n",
            "File \u001b[0;32m~/Code_Workspace/Python/CodeSematicSearch/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Code_Workspace/Python/CodeSematicSearch/venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
            "File \u001b[0;32m~/Code_Workspace/Python/CodeSematicSearch/venv/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ],
      "source": [
        "\"\"\" if load_model:\n",
        "    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer) \"\"\"\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
        "\n",
        "    if save_model:\n",
        "        checkpoint = {\n",
        "            \"state_dict\": model.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "        }\n",
        "        #save_checkpoint(checkpoint)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    model.train()\n",
        "    losses = []\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        print(\"batch idx \",batch_idx)\n",
        "        # Get input and targets and get to cuda\n",
        "        #print(batch)\n",
        "        inp_data,target = batch\n",
        "        #inp_data = inp_data.to(device)\n",
        "        #target = target.to(device)\n",
        "\n",
        "        \n",
        "        print(inp_data.shape,target.shape)\n",
        "        print(inp_data.shape[0])\n",
        "\n",
        "        #print(torch.full((512-inp_data.shape[0],1),3 ))\n",
        "\n",
        "        inp_data = torch.cat( (inp_data, torch.full((512-inp_data.shape[0],1),3 )),0)\n",
        "        target = torch.cat( (target, torch.full((512-target.shape[0],1),3)),0 )\n",
        "\n",
        "        print(inp_data.shape,target.shape)\n",
        "        target[:-1, :]\n",
        "        output = model(inp_data,target )\n",
        "        print(\"output:\",output.shape)\n",
        "\n",
        "        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n",
        "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
        "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
        "        # way that we have output_words * batch_size that we want to send in into\n",
        "        # our cost function, so we need to do some reshapin.\n",
        "        # Let's also remove the start token while we're at it\n",
        "        output = output.reshape(-1, output.shape[2])\n",
        "        target = target[1:].reshape(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # Back prop\n",
        "        loss.backward()\n",
        "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
        "        # within a healthy range\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "        # Gradient descent step\n",
        "        optimizer.step()\n",
        "\n",
        "        # plot to tensorboard\n",
        "        writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
        "        step += 1\n",
        "\n",
        "    mean_loss = sum(losses) / len(losses)\n",
        "    scheduler.step(mean_loss)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "4bbd86816b1ffb67a152ca615b009f4e93e61f6a1bcfd70f5e481c2257c072b5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
